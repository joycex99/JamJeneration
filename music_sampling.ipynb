{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JAM JENERATION\n",
    "\n",
    "Starter code taken from this blog post on [jazz improvisation](https://www.hackerearth.com/blog/machine-learning/jazz-music-using-deep-learning/) with [Github repo](https://github.com/shubham3121/music-generation-using-rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re \n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from glob import glob\n",
    "import IPython\n",
    "import pickle\n",
    "\n",
    "import music21\n",
    "\n",
    "# import play # ERIC: This is me being dumb, doesn't work for me, skipping\n",
    "# Note: play needs to be imported from pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import converter, instrument, note, chord, stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of songs: 26\n",
      "['Beethoven/waldstein_1.mid', 'Beethoven/beethoven_opus90_2.mid', 'Beethoven/waldstein_2.mid', 'Beethoven/beethoven_opus90_1.mid', 'Beethoven/waldstein_3.mid']\n"
     ]
    }
   ],
   "source": [
    "# Any directory with .mid files in here is acceptable. We have 3 in Jazz and 26 in Beethoven.\n",
    "\n",
    "songs = glob('Beethoven/*.mid')\n",
    "print(\"Number of songs: {}\".format(len(songs)))\n",
    "print(songs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline: Logistic Regression\n",
    "\n",
    "In this super simple baseline, we ignore chords and pretend that all songs are just sequences of individual notes. Given a window of previous notes, we attempt to predict the next one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Notes Simply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_notes():\n",
    "    \"\"\"\n",
    "    Returns a list of notes comprising our music.\n",
    "    For chords in our piece, return the note of our top note.\n",
    "    i.e. [F#5, C#7, C5, etc.]\n",
    "    \"\"\"\n",
    "    notes = []\n",
    "    for file in songs:\n",
    "        # converting .mid file to stream object\n",
    "        midi = converter.parse(file)\n",
    "        notes_to_parse = []\n",
    "        try:\n",
    "            # Given a single stream, partition into a part for each unique instrument\n",
    "            parts = instrument.partitionByInstrument(midi)\n",
    "        except:\n",
    "            pass\n",
    "        if parts: # if parts has instrument parts \n",
    "            notes_to_parse = parts.parts[0].recurse()\n",
    "        else:\n",
    "            notes_to_parse = midi.flat.notes\n",
    "    \n",
    "        for element in notes_to_parse: \n",
    "            if isinstance(element, note.Note):\n",
    "                # if element is a note, extract pitch\n",
    "                notes.append(str(element.pitch))\n",
    "            elif(isinstance(element, chord.Chord)):\n",
    "                # if element is a chord, append the first note\n",
    "                notes.append(str(element.pitches[0]))\n",
    "        print(\"Processed song {}\".format(file))\n",
    "    with open('data/simple_notes', 'wb') as filepath:\n",
    "        pickle.dump(notes, filepath)\n",
    "    \n",
    "    return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed song Beethoven/waldstein_1.mid\n",
      "Processed song Beethoven/beethoven_opus90_2.mid\n",
      "Processed song Beethoven/waldstein_2.mid\n",
      "Processed song Beethoven/beethoven_opus90_1.mid\n",
      "Processed song Beethoven/waldstein_3.mid\n",
      "Processed song Beethoven/beethoven_opus10_2.mid\n",
      "Processed song Beethoven/beethoven_opus10_3.mid\n",
      "Processed song Beethoven/elise.mid\n",
      "Processed song Beethoven/beethoven_opus10_1.mid\n",
      "Processed song Beethoven/beethoven_les_adieux_3.mid\n",
      "Processed song Beethoven/pathetique_1.mid\n",
      "Processed song Beethoven/mond_1.mid\n",
      "Processed song Beethoven/beethoven_les_adieux_2.mid\n",
      "Processed song Beethoven/mond_3.mid\n",
      "Processed song Beethoven/pathetique_2.mid\n",
      "Processed song Beethoven/pathetique_3.mid\n",
      "Processed song Beethoven/mond_2.mid\n",
      "Processed song Beethoven/beethoven_les_adieux_1.mid\n",
      "Processed song Beethoven/beethoven_opus22_1.mid\n",
      "Processed song Beethoven/beethoven_opus22_2.mid\n",
      "Processed song Beethoven/beethoven_hammerklavier_4.mid\n",
      "Processed song Beethoven/beethoven_opus22_3.mid\n",
      "Processed song Beethoven/beethoven_hammerklavier_1.mid\n",
      "Processed song Beethoven/beethoven_opus22_4.mid\n",
      "Processed song Beethoven/beethoven_hammerklavier_3.mid\n",
      "Processed song Beethoven/beethoven_hammerklavier_2.mid\n",
      "Number of notes in dataset: 74603\n",
      "['C2', 'C3', 'G2', 'C3', 'G2', 'C3', 'G2', 'C3', 'G2', 'C3']\n"
     ]
    }
   ],
   "source": [
    "simple_notes = get_simple_notes()\n",
    "print(\"Number of notes in dataset: {}\".format(len(simple_notes)))\n",
    "print(simple_notes[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct notes in dataset: 78\n",
      "[('C4', 2410), ('G3', 2373), ('E-4', 2266)]\n",
      "Guess-most-common classifier accuracy: 0.03230433092502982\n"
     ]
    }
   ],
   "source": [
    "simple_note_counts = Counter(simple_notes)\n",
    "print(\"Number of distinct notes in dataset: {}\".format(len(simple_note_counts)))\n",
    "print(simple_note_counts.most_common(3))\n",
    "print(\"Guess-most-common classifier accuracy: {}\".format(simple_note_counts.most_common(1)[0][1]/len(simple_notes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Logistic Regression Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(note, note_to_int):\n",
    "    \"\"\" Returns one-hot encoded vector given note, dictionary from notes to indices \"\"\"\n",
    "    n_vocab = len(note_to_int)\n",
    "    one_hot = np.zeros(n_vocab)\n",
    "    note_idx = note_to_int[note]\n",
    "    one_hot[note_idx] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_simple_sequences(notes, sequence_length):\n",
    "    \"\"\"\n",
    "    Prepares vectors of simple notes for one-hot encoding input into LogisticRegression classifer\n",
    "    \n",
    "    returns X, y\n",
    "    X: a list of training examples, where each training example are concatenations of one-hot encodings\n",
    "        Each training input in X is thus (sequence_length * n_vocab) in length\n",
    "    y: a list of notes. Each note corresponds to the next note in corresponding sequence from X\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract the unique pitches in the list of notes.\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "    n_vocab = len(pitchnames)\n",
    "\n",
    "    # Create a dictionary to map pitches to integers\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    # Slide a window over our notes, adding sequences to dataset\n",
    "    X, y = [], []\n",
    "    for i in range(0, len(notes) - sequence_length):\n",
    "        sequence_in = notes[i : i + sequence_length]\n",
    "        sequence_in = [one_hot_encoding(n, note_to_int) for n in sequence_in]\n",
    "        sequence_in = np.concatenate(sequence_in, axis=None)\n",
    "        note_out = notes[i + sequence_length]\n",
    "        X.append(sequence_in)\n",
    "        y.append(note_out)\n",
    "    \n",
    "    # TODO: Should we turn our labels into categorical one-hot encodings using np_utils.to_categorical\n",
    "    # TODO: Should we normalize input?\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence length: 10\n",
      "Number of distinct notes: 78\n",
      "Number of training examples: 74593\n",
      "First five output notes: ['G2', 'C3', 'G2', 'C3', 'G2']\n"
     ]
    }
   ],
   "source": [
    "simple_sequence_length = 10\n",
    "simple_X, simple_y = prepare_simple_sequences(simple_notes, simple_sequence_length)\n",
    "print(\"Sequence length: {}\".format(simple_sequence_length))\n",
    "print(\"Number of distinct notes: {}\".format(len(set(simple_notes))))\n",
    "print(\"Number of training examples: {}\".format(len(simple_X)))\n",
    "print(\"First five output notes: {}\".format(simple_y[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the following train-test split procedure could be somewhat flawed. We don't split on unseen (unheard) songs, so a melody that appears multiple times in one song could be picked up later on in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 63404\n",
      "Test set size: 11189\n"
     ]
    }
   ],
   "source": [
    "simple_X_train, simple_X_test, simple_y_train, simple_y_test = train_test_split(simple_X, simple_y, test_size=0.15)\n",
    "simple_y_train[:5]\n",
    "print(\"Train set size: {}\".format(len(simple_y_train)))\n",
    "print(\"Test set size: {}\".format(len(simple_y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Simple Logistic Regression Multi-Class Classification Model\n",
    "\n",
    "In this simpler problem, we assume that each sequence of notes has one and only one note succeeding it. This makes it a multi-class classification problem.\n",
    "\n",
    "Later we'll have to expand it to multi-label problem for chords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/etang/dev/jam-jeneration/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/etang/dev/jam-jeneration/venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(simple_X_train, simple_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35624273840378945"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_preds_test = logreg.predict(simple_X_test)\n",
    "sklearn.metrics.accuracy_score(simple_preds_test, simple_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Chords and Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_notes():\n",
    "    notes = []\n",
    "    for file in songs:\n",
    "        # converting .mid file to stream object\n",
    "        midi = converter.parse(file)\n",
    "        notes_to_parse = []\n",
    "        try:\n",
    "            # Given a single stream, partition into a part for each unique instrument\n",
    "            parts = instrument.partitionByInstrument(midi)\n",
    "        except:\n",
    "            pass\n",
    "        if parts: # if parts has instrument parts \n",
    "            notes_to_parse = parts.parts[0].recurse()\n",
    "        else:\n",
    "            notes_to_parse = midi.flat.notes\n",
    "    \n",
    "        for element in notes_to_parse: \n",
    "            if isinstance(element, note.Note):\n",
    "                # if element is a note, extract pitch\n",
    "                notes.append(str(element.pitch))\n",
    "            elif(isinstance(element, chord.Chord)):\n",
    "                # if element is a chord, append the normal form of the \n",
    "                # chord (a list of integers) to the list of notes. \n",
    "                notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "    with open('data/notes', 'wb') as filepath:\n",
    "        pickle.dump(notes, filepath)\n",
    "    \n",
    "    return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['G3', 'F2', 'F3', 'E2', '10.2.4', 'E3', '7.10.0', '0', '10.0.4', '7.10.0']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_notes()[145:155]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Sequence Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_sequences(notes, n_vocab): \n",
    "    sequence_length = 100\n",
    "\n",
    "    # Extract the unique pitches in the list of notes.\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "\n",
    "    # Create a dictionary to map pitches to integers\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    network_input = []\n",
    "    network_output = []\n",
    "\n",
    "    # create input sequences and the corresponding outputs\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i: i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "        network_output.append(note_to_int[sequence_out])\n",
    "    \n",
    "    n_patterns = len(network_input)\n",
    "    \n",
    "    # reshape the input into a format comatible with LSTM layers \n",
    "    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "    \n",
    "    # normalize input\n",
    "    network_input = network_input / float(n_vocab)\n",
    "    \n",
    "    # one hot encode the output vectors\n",
    "    network_output = np_utils.to_categorical(network_output)\n",
    "    \n",
    "    return (network_input, network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, LSTM, Dropout, Flatten\n",
    "def create_network(network_in, n_vocab): \n",
    "    \"\"\"Create the model architecture\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=(100,1), return_sequences=True)) # network_in.shape[1:]\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(n_vocab))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "#     print(network_in.shape[1:])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "def train(model, network_input, network_output, epochs): \n",
    "    \"\"\"\n",
    "    Train the neural network\n",
    "    \"\"\"\n",
    "    # Create checkpoint to save the best model weights.\n",
    "    filepath = 'weights.best.music3.hdf5'\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True)\n",
    "    \n",
    "    model.fit(network_input, network_output, epochs=epochs, batch_size=32, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_network():\n",
    "    \"\"\"\n",
    "    Get notes\n",
    "    Generates input and output sequences\n",
    "    Creates a model \n",
    "    Trains the model for the given epochs\n",
    "    \"\"\"\n",
    "    \n",
    "    epochs = 50\n",
    "    \n",
    "    notes = get_notes()\n",
    "    print('Notes processed')\n",
    "    \n",
    "    n_vocab = len(set(notes))\n",
    "    print('Vocab generated')\n",
    "    \n",
    "    network_in, network_out = prepare_sequences(notes, n_vocab)\n",
    "    print('Input and Output processed')\n",
    "    \n",
    "    model = create_network(network_in, n_vocab)\n",
    "    print('Model created')\n",
    "#     return model\n",
    "    print('Training in progress')\n",
    "    train(model, network_in, network_out, epochs)\n",
    "    print('Training completed')\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notes processed\n",
      "Vocab generated\n",
      "Input and Output processed\n",
      "Model created\n",
      "Training in progress\n",
      "Epoch 1/50\n",
      "632/632 [==============================] - 7s 11ms/step - loss: 4.0883\n",
      "Epoch 2/50\n",
      "632/632 [==============================] - 5s 8ms/step - loss: 3.6452\n",
      "Epoch 3/50\n",
      "632/632 [==============================] - 5s 7ms/step - loss: 3.4801\n",
      "Epoch 4/50\n",
      "632/632 [==============================] - 5s 7ms/step - loss: 3.3889\n",
      "Epoch 5/50\n",
      "632/632 [==============================] - 5s 7ms/step - loss: 3.2978\n",
      "Epoch 6/50\n",
      "632/632 [==============================] - 5s 7ms/step - loss: 3.2238\n",
      "Epoch 7/50\n",
      "632/632 [==============================] - 5s 8ms/step - loss: 3.1480\n",
      "Epoch 8/50\n",
      "632/632 [==============================] - 5s 8ms/step - loss: 3.0585\n",
      "Epoch 9/50\n",
      "632/632 [==============================] - 5s 8ms/step - loss: 2.9736\n",
      "Epoch 10/50\n",
      "632/632 [==============================] - 5s 8ms/step - loss: 2.9379\n",
      "Epoch 11/50\n",
      "632/632 [==============================] - 5s 7ms/step - loss: 2.8721\n",
      "Epoch 12/50\n",
      "632/632 [==============================] - 5s 8ms/step - loss: 2.8011\n",
      "Epoch 13/50\n",
      "632/632 [==============================] - 5s 8ms/step - loss: 2.7385\n",
      "Epoch 14/50\n",
      "632/632 [==============================] - 5s 8ms/step - loss: 2.7061\n",
      "Epoch 15/50\n",
      "632/632 [==============================] - 5s 8ms/step - loss: 2.6192\n",
      "Epoch 16/50\n",
      "632/632 [==============================] - 5s 7ms/step - loss: 2.5448\n",
      "Epoch 17/50\n",
      "632/632 [==============================] - 5s 7ms/step - loss: 2.3793\n",
      "Epoch 18/50\n",
      "632/632 [==============================] - 5s 8ms/step - loss: 2.3556\n",
      "Epoch 19/50\n",
      "632/632 [==============================] - 6s 9ms/step - loss: 2.2337\n",
      "Epoch 20/50\n",
      "632/632 [==============================] - 6s 10ms/step - loss: 2.1257\n",
      "Epoch 21/50\n",
      "632/632 [==============================] - 5s 8ms/step - loss: 1.9678\n",
      "Epoch 22/50\n",
      "632/632 [==============================] - 5s 8ms/step - loss: 1.8232\n",
      "Epoch 23/50\n",
      "632/632 [==============================] - 5s 7ms/step - loss: 1.7921\n",
      "Epoch 24/50\n",
      "632/632 [==============================] - 4s 7ms/step - loss: 1.5969\n",
      "Epoch 25/50\n",
      "632/632 [==============================] - 5s 7ms/step - loss: 1.5500\n",
      "Epoch 26/50\n",
      "632/632 [==============================] - 5s 8ms/step - loss: 1.4538\n",
      "Epoch 27/50\n",
      "632/632 [==============================] - 5s 8ms/step - loss: 1.3952\n",
      "Epoch 28/50\n",
      "632/632 [==============================] - 5s 8ms/step - loss: 1.3082\n",
      "Epoch 29/50\n",
      "632/632 [==============================] - 5s 8ms/step - loss: 1.1772\n",
      "Epoch 30/50\n",
      "632/632 [==============================] - 5s 7ms/step - loss: 1.0515\n",
      "Epoch 31/50\n",
      "632/632 [==============================] - 5s 7ms/step - loss: 1.0113\n",
      "Epoch 32/50\n",
      "632/632 [==============================] - 5s 8ms/step - loss: 0.9338\n",
      "Epoch 33/50\n",
      "632/632 [==============================] - 5s 8ms/step - loss: 0.8060\n",
      "Epoch 34/50\n",
      "632/632 [==============================] - 5s 8ms/step - loss: 0.7533\n",
      "Epoch 35/50\n",
      "632/632 [==============================] - 5s 7ms/step - loss: 0.5816\n",
      "Epoch 36/50\n",
      "632/632 [==============================] - 5s 7ms/step - loss: 0.5690\n",
      "Epoch 37/50\n",
      "632/632 [==============================] - 5s 7ms/step - loss: 0.4832\n",
      "Epoch 38/50\n",
      "632/632 [==============================] - 5s 8ms/step - loss: 0.4077\n",
      "Epoch 39/50\n",
      "632/632 [==============================] - 5s 7ms/step - loss: 0.3622\n",
      "Epoch 40/50\n",
      "632/632 [==============================] - 4s 7ms/step - loss: 0.3601\n",
      "Epoch 41/50\n",
      "632/632 [==============================] - 5s 7ms/step - loss: 0.3309\n",
      "Epoch 42/50\n",
      "632/632 [==============================] - 5s 7ms/step - loss: 0.3275\n",
      "Epoch 43/50\n",
      "632/632 [==============================] - 4s 7ms/step - loss: 0.2647\n",
      "Epoch 44/50\n",
      "632/632 [==============================] - 4s 7ms/step - loss: 0.2078\n",
      "Epoch 45/50\n",
      "632/632 [==============================] - 4s 7ms/step - loss: 0.2075\n",
      "Epoch 46/50\n",
      "632/632 [==============================] - 4s 7ms/step - loss: 0.2023\n",
      "Epoch 47/50\n",
      "632/632 [==============================] - 4s 7ms/step - loss: 0.1868\n",
      "Epoch 48/50\n",
      "632/632 [==============================] - 4s 7ms/step - loss: 0.1659\n",
      "Epoch 49/50\n",
      "632/632 [==============================] - 5s 7ms/step - loss: 0.1681\n",
      "Epoch 50/50\n",
      "632/632 [==============================] - 5s 7ms/step - loss: 0.1694\n",
      "Training completed\n"
     ]
    }
   ],
   "source": [
    "### Train the model \n",
    "model = train_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(model=None):\n",
    "    \"\"\" Generate a piano midi file \"\"\"\n",
    "    #load the notes used to train the model\n",
    "    with open('data/notes', 'rb') as filepath:\n",
    "        notes = pickle.load(filepath)\n",
    "\n",
    "    # Get all pitch names\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "    # Get all pitch names\n",
    "    n_vocab = len(set(notes))\n",
    "    \n",
    "    print('Initiating music generation process.......')\n",
    "    \n",
    "    network_input = get_inputSequences(notes, pitchnames, n_vocab)\n",
    "    \n",
    "#             reshaped_input = np.reshape(pattern, (1, len(pattern), 1))\n",
    "#         prediction_input = prediction_input / float(n_vocab)\n",
    "        \n",
    "    if not model:\n",
    "        model = create_network(network_input, n_vocab)\n",
    "        print('Loading Model weights.....')\n",
    "        model.load_weights('weights.best.music3.hdf5')\n",
    "    else:\n",
    "        print('Using given model')\n",
    "    \n",
    "    print('Model Loaded')\n",
    "    prediction_output = generate_notes(model, network_input, pitchnames, n_vocab)\n",
    "    create_midi(prediction_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inputSequences(notes, pitchnames, n_vocab):\n",
    "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
    "    # map between notes and integers and back\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    sequence_length = 100\n",
    "    network_input = []\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "\n",
    "    return (network_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_notes(model, network_input, pitchnames, n_vocab):\n",
    "    \"\"\" Generate notes from the neural network based on a sequence of notes \"\"\"\n",
    "    # Pick a random integer\n",
    "    start = np.random.randint(0, len(network_input)-1)\n",
    "\n",
    "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
    "    \n",
    "    # pick a random sequence from the input as a starting point for the prediction\n",
    "    pattern = network_input[start]\n",
    "    prediction_output = []\n",
    "    \n",
    "    print('Generating notes........')\n",
    "\n",
    "    # generate 500 notes\n",
    "    for note_index in range(500):\n",
    "        prediction_input = np.reshape(pattern, (1, len(pattern), 1))\n",
    "        prediction_input = prediction_input / float(n_vocab)\n",
    "\n",
    "        prediction = model.predict(prediction_input, verbose=0)\n",
    "        \n",
    "        # Predicted output is the argmax(P(h|D))\n",
    "        index = np.argmax(prediction)\n",
    "        # Mapping the predicted interger back to the corresponding note\n",
    "        result = int_to_note[index]\n",
    "        # Storing the predicted output\n",
    "        prediction_output.append(result)\n",
    "\n",
    "        pattern.append(index)\n",
    "        # Next input to the model\n",
    "        pattern = pattern[1:len(pattern)]\n",
    "\n",
    "    print('Notes Generated...')\n",
    "    return prediction_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_midi(prediction_output):\n",
    "    \"\"\" convert the output from the prediction to notes and create a midi file\n",
    "        from the notes \"\"\"\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "    for pattern in prediction_output:\n",
    "        # pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                new_note = note.Note(int(current_note))\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "        # pattern is a note\n",
    "        else:\n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "\n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        offset += 0.5\n",
    "\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "    \n",
    "    print('Saving Output file as midi....')\n",
    "\n",
    "    midi_stream.write('midi', fp='test_output4.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating music generation process.......\n",
      "Using given model\n",
      "Model Loaded\n",
      "Generating notes........\n",
      "Notes Generated...\n",
      "Saving Output file as midi....\n"
     ]
    }
   ],
   "source": [
    "#### Generate a new jazz music \n",
    "generate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Music file test_output4.mid loaded!\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2870: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "### Play the Jazz music\n",
    "play.play_midi('test_output4.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
